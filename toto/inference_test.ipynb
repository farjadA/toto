{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from data.util.dataset import MaskedTimeseries\n",
    "from inference.forecaster import TotoForecaster\n",
    "from model.toto import Toto\n",
    "\n",
    "# DEVICE = 'cuda'\n",
    "DEVICE = 'cpu'\n",
    "\n",
    "# Load pre-trained Toto model\n",
    "toto = Toto.from_pretrained('Datadog/Toto-Open-Base-1.0').to(DEVICE)\n",
    "\n",
    "# Optional: compile model for enhanced speed\n",
    "toto.compile()\n",
    "\n",
    "forecaster = TotoForecaster(toto.model)\n",
    "\n",
    "# Example input series (7 variables, 4096 timesteps)\n",
    "input_series = torch.randn(7, 4096).to(DEVICE)\n",
    "timestamp_seconds = torch.zeros(7, 4096).to(DEVICE)\n",
    "time_interval_seconds = torch.full((7,), 60*15).to(DEVICE)\n",
    "\n",
    "inputs = MaskedTimeseries(\n",
    "    series=input_series,\n",
    "    padding_mask=torch.full_like(input_series, True, dtype=torch.bool),\n",
    "    id_mask=torch.zeros_like(input_series),\n",
    "    timestamp_seconds=timestamp_seconds,\n",
    "    time_interval_seconds=time_interval_seconds,\n",
    ")\n",
    "\n",
    "# Generate forecasts for next 336 timesteps\n",
    "forecast = forecaster.forecast(\n",
    "    inputs,\n",
    "    prediction_length=336,\n",
    "    num_samples=256,\n",
    "    samples_per_batch=256,\n",
    ")\n",
    "\n",
    "# Access results\n",
    "median_prediction = forecast.median\n",
    "prediction_samples = forecast.samples\n",
    "lower_quantile = forecast.quantile(0.1)\n",
    "upper_quantile = forecast.quantile(0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7, 336])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "median_prediction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Toto(\n",
       "  (model): TotoBackbone(\n",
       "    (patch_embed): PatchEmbedding(\n",
       "      (projection): Linear(in_features=64, out_features=768, bias=True)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (rotary_emb): TimeAwareRotaryEmbedding()\n",
       "      (layers): ModuleList(\n",
       "        (0-10): 11 x TransformerLayer(\n",
       "          (norm1): RMSNorm()\n",
       "          (norm2): RMSNorm()\n",
       "          (attention): TimeWiseMultiheadAttention(\n",
       "            (rotary_emb): TimeAwareRotaryEmbedding()\n",
       "            (wQKV): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (wO): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (mlp): Sequential(\n",
       "            (0): Linear(in_features=768, out_features=6144, bias=True)\n",
       "            (1): SwiGLU()\n",
       "            (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (3): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): TransformerLayer(\n",
       "          (norm1): RMSNorm()\n",
       "          (norm2): RMSNorm()\n",
       "          (attention): SpaceWiseMultiheadAttention(\n",
       "            (wQKV): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (wO): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (mlp): Sequential(\n",
       "            (0): Linear(in_features=768, out_features=6144, bias=True)\n",
       "            (1): SwiGLU()\n",
       "            (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (3): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (unembed): Linear(in_features=768, out_features=49152, bias=True)\n",
       "    (output_distribution): MixtureOfStudentTsOutput(\n",
       "      (df): Linear(in_features=768, out_features=24, bias=True)\n",
       "      (loc_proj): Linear(in_features=768, out_features=24, bias=True)\n",
       "      (scale_proj): Linear(in_features=768, out_features=24, bias=True)\n",
       "      (mixture_weights): Linear(in_features=768, out_features=24, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now on "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "toto",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
